{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Many thanks to the 2016 Decemberist class and the many insightful comments.\n",
    "Due to the lack of a joystick this model was trained entirely off the test data set provide by Udacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Data exploration\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "#Load data\n",
    "d_f = pd.read_csv('data/driving_log.csv')\n",
    "\n",
    "def hist_plot(d_f):\n",
    "    print(\"Size of data set\", len(d_f))\n",
    "    #Get steering and accelaration details\n",
    "    steer_rad= d_f['steering'].tolist()\n",
    "    throttle = d_f['throttle'].tolist()\n",
    "    #Mean\n",
    "    ave_steer = np.mean(steer_rad)\n",
    "    var_steer = np.var(steer_rad)\n",
    "    #Variance\n",
    "    ave_throt = np.mean(throttle)\n",
    "    var_throt =np.var(throttle)\n",
    "\n",
    "    #Plot histograms\n",
    "    f, axarr = plt.subplots(2, 1)\n",
    "    axarr[0].hist(steer_rad, bins=100),axarr[0].set_title(\"Steering angle Histogram: Mean angle: {0:.4f}, Var: {1:.4f}\".format(ave_steer, var_steer)),axarr[0].set_xlabel(\"Radians\"), axarr[0].set_ylabel(\"Frequency\")\n",
    "    axarr[1].hist(throttle, bins=50),axarr[1].set_title(\"Throttle Histogram: Mean accel: {0:.4f}, Var: {1:.4f}\".format(ave_throt, var_throt)),axarr[1].set_xlabel(\"Units\"), axarr[1].set_ylabel(\"Frequency\")\n",
    "    plt.tight_layout(pad=0.2, w_pad=0.1, h_pad=0.1)\n",
    "    plt.show()\n",
    "    %matplotlib inline \n",
    "#Display histograms\n",
    "hist_plot(d_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Count number of 0.0 steering angles\n",
    "acc_zero = d_f['steering']\n",
    "non_zero  = np.count_nonzero(acc_zero)\n",
    "total_zero = len(acc_zero) - non_zero\n",
    "print(\"Total number of zero acceleration entries: {0}, total number of other entries: {1}\".format(total_zero, non_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the above frequencies, the data set is very skewed towards xero angles and right turns. To balance out the data set, 80% of all these values are dropped so as to stop the model to-be from being biased towards this values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Drop 90% of zero angle data\n",
    "drop = int(0.2*total_zero)\n",
    "\n",
    "#Get non-zero and zero entries\n",
    "df_n = d_f[d_f.steering != 0]\n",
    "df_z = d_f[d_f.steering == 0]\n",
    "df_z = df_z.ix[:drop]\n",
    "\n",
    "#Augment new entries with non-zero angle entries\n",
    "new_dataset = pd.concat([df_z, df_n])\n",
    "hist_plot(new_dataset)\n",
    "#new_dataset = d_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "#train, test = train_test_split(new_dataset, test_size = 0.10)\n",
    "train = new_dataset\n",
    "test = new_dataset\n",
    "#Display new data set stats\n",
    "hist_plot(train)\n",
    "hist_plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation, ELU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import matplotlib.image as mpimg\n",
    "#Load data\n",
    "d_f = pd.read_csv('data/driving_log.csv')\n",
    "ANGLE_OFFSET = 0.25\n",
    "\n",
    "# Angle offsets applied to center, left and right image\n",
    "ANGLE_OFFSETS = [0.0, ANGLE_OFFSET, -ANGLE_OFFSET]\n",
    "\n",
    "# Batch size\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "def preprocess_input(x):\n",
    "    FINAL_IMG_SHAPE = (66, 200, 3)\n",
    "\n",
    "    height = x.shape[0]\n",
    "    width = x.shape[1]\n",
    "\n",
    "    factor = float(FINAL_IMG_SHAPE[1]) / float(width)\n",
    "\n",
    "    resized_size = (int(width*factor), int(height*factor))\n",
    "    x = cv2.resize(x, resized_size)\n",
    "    crop_height = resized_size[1] - FINAL_IMG_SHAPE[0]\n",
    "\n",
    "    return x[crop_height:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def random_horizontal_flip(x, y):\n",
    "    flip = np.random.randint(2)\n",
    "\n",
    "    if flip:\n",
    "        x = cv2.flip(x, 1)\n",
    "        y = -y\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def random_translation(img, steering):\n",
    "    # Maximum shift of the image, in pixels\n",
    "    trans_range = 50  # Pixels\n",
    "\n",
    "    # Compute translation and corresponding steering angle\n",
    "    tr_x = np.random.uniform(-trans_range, trans_range)\n",
    "    steering = steering + (tr_x / trans_range) * ANGLE_OFFSET\n",
    "\n",
    "    # Warp image using the computed translation\n",
    "    rows = img.shape[0]\n",
    "    cols = img.shape[1]\n",
    "\n",
    "    M = np.float32([[1,0,tr_x],[0,1,0]])\n",
    "    img = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "    return img, steering\n",
    "\n",
    "def data_augmentation(x, y):\n",
    "    # Random horizontal shift\n",
    "    x, y = random_translation(x, y)\n",
    "\n",
    "    # Random flip\n",
    "    x, y = random_horizontal_flip(x, y)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a limited data set, I augmented the data set with random translations to make the model robust to randomness. Additionally given that there are more right turns than left, the image data set is also reflected to make the training data more symetrical.\n",
    "\n",
    "These data augmentation functions are then passed onto generator functions as the combined augmented data set is over the memory limits of the AWS GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_generator(X,y, batch_size):\n",
    "    \"\"\" Provides a batch of images from a log file. The main advantage\n",
    "        of using a generator is that we do not need to read the whole log file,\n",
    "        only one batch at a time, so it will fit in RAM.\n",
    "        This function also generates extended data on the fly. \"\"\"\n",
    "    # Supply training images indefinitely\n",
    "    while 1:\n",
    "        # Declare output data\n",
    "        x_out = []\n",
    "        y_out = []\n",
    "        \n",
    "        # Fill batch\n",
    "        for i in range(0, batch_size):\n",
    "            # Get random index to an element in the dataset.\n",
    "            idx = np.random.randint(len(y))\n",
    "\n",
    "            # Randomly select which of the 3 images (center, left, right) to use\n",
    "            idx_img = np.random.randint(len(ANGLE_OFFSETS))\n",
    "\n",
    "            # Read image and steering angle (with added offset)\n",
    "            x_i = mpimg.imread(\"data/\"+X[idx][idx_img].strip())\n",
    "            y_i = y[idx] + ANGLE_OFFSETS[idx_img]\n",
    "\n",
    "            # Preprocess image\n",
    "            x_i = preprocess_input(x_i)\n",
    "\n",
    "            # Augment data\n",
    "            x_i, y_i = data_augmentation(x_i, y_i)\n",
    "\n",
    "            # Add to batch\n",
    "            x_out.append(x_i)\n",
    "            y_out.append(y_i)\n",
    "\n",
    "        yield (np.array(x_out), np.array(y_out))\n",
    "\n",
    "\n",
    "def val_generator(X, y):\n",
    "    \"\"\" Provides images for validation. This generator is different\n",
    "        from the previous one in that it does **not** perform data augmentation:\n",
    "        it just reads images from disk, preprocess them and yields them \"\"\"\n",
    "    # Validation generator\n",
    "    while 1:\n",
    "        for i in range(len(y)):\n",
    "            # Read image and steering angle\n",
    "            x_out = mpimg.imread(\"data/\"+X[i][0].strip())\n",
    "            y_out = np.array([[y[i]]])\n",
    "\n",
    "            # Preprocess image\n",
    "            x_out = preprocess_input(x_out)\n",
    "            x_out = x_out[None, :, :, :]\n",
    "\n",
    "            # Return the data\n",
    "            yield x_out, y_out\n",
    "\n",
    "\n",
    "def make_multiple(x, number):\n",
    "    \"\"\" Increases x to be the smallest multiple of number \"\"\"\n",
    "    return int(math.ceil(float(x) / float(number)) * number)\n",
    "\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\" Normalizes the input between -0.5 and 0.5 \"\"\"\n",
    "    return X / 255. - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemented neural network is based on the NVIDIA architecture from  http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Defines the network architecture, following Nvidia's example on:\n",
    "        http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf \"\"\"\n",
    "\n",
    "# Parameters\n",
    "FINAL_IMG_SHAPE = (66, 200, 3)\n",
    "\n",
    "input_shape = FINAL_IMG_SHAPE\n",
    "\n",
    "weight_init='glorot_uniform'\n",
    "padding = 'valid'\n",
    "dropout_prob = 0.25\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(normalize, input_shape=input_shape, output_shape=input_shape))\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5,\n",
    "                            border_mode=padding,\n",
    "                            init = weight_init, subsample = (2, 2)))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(36, 5, 5,\n",
    "                            border_mode=padding,\n",
    "                            init = weight_init, subsample = (2, 2)))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(48, 5, 5,\n",
    "                            border_mode=padding,\n",
    "                            init = weight_init, subsample = (2, 2)))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3,\n",
    "                            border_mode=padding,\n",
    "                            init = weight_init, subsample = (1, 1)))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3,\n",
    "                            border_mode=padding,\n",
    "                            init = weight_init, subsample = (1, 1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout_prob))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(100, init = weight_init))\n",
    "model.add(Dropout(dropout_prob))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(50, init = weight_init))\n",
    "model.add(Dropout(dropout_prob))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(10, init = weight_init))\n",
    "model.add(Dropout(dropout_prob))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(1, init = weight_init, name = 'output'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile it\n",
    "model.compile(loss = 'mse', optimizer = Adam(lr = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_training_data(data):\n",
    "    X = np.column_stack((np.copy(data['center']), np.copy(data['left']), np.copy(data['right'])))\n",
    "    y = np.copy(data['steering'])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "X_train, y_train =  get_training_data(train)\n",
    "X_test, y_test =  get_training_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_train_samples = 5 * make_multiple(len(y_train), batch_size)\n",
    "n_val_samples = len(y_test)\n",
    "n_epochs = 5\n",
    "gen_train = train_generator(X_train, y_train, batch_size)\n",
    "gen_val = val_generator(X_test, y_test)\n",
    "\n",
    "\n",
    "model.fit_generator(generator = gen_train,\n",
    "                        samples_per_epoch = n_train_samples,\n",
    "                        validation_data = gen_val,\n",
    "                        nb_val_samples = n_val_samples,\n",
    "                        nb_epoch = n_epochs,\n",
    "                       \n",
    "                        verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Save final model as specified in Udacity ruberic\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../../model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save('../../model.h5') \n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
